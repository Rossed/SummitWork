{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with the 20-Newsgroups Dataset\n",
    "Authors: R. Edwards, J.Giles, N. Velzboer, E. Whitney and you! <br>\n",
    "Purpose: Exploring different clustering approaches with the 20 newsgroup dataset, comprising of user questions and posts on different web forums (open source: http://qwone.com/~jason/20Newsgroups/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Relavent Libraries\n",
    "\n",
    "## Python libraries \n",
    "import string\n",
    "import time\n",
    "\n",
    "## Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Plotting Data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Feature selection\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "## Dimensionality Reduction \n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "## Clustering \n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "## Get rid of pesky warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "Reading in data, creating pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn maintains offers this data as part of their inbuilt learning sets.\n",
    "#  We'll download the 'training' set,  however ignore the name 'train'. They \n",
    "# offer the data split into training and testing sets if you are trying to apply some classification approach \n",
    "# May take up to a minute to download\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data into pandas dataframe. Non essential, but pandas dataframes are nice to work with\n",
    "df_text_ng=pd.DataFrame(data={'text':newsgroups_train.data}, columns=['text'])\n",
    "\n",
    "# Take subset of dataset to prevent memory/kernel/processing issues\n",
    "row_lim = 20\n",
    "df_text_ng = df_text_ng.iloc[0:row_lim,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and pre-processing\n",
    "We will apply the following pre-processing steps <br>\n",
    "<ol>\n",
    "<li> Remove unwanted characters\n",
    "<li> Tokenisation and case sensitive \n",
    "<li> Stopwords\n",
    "<li> Lemmitisation \n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard NLTK stopwords list\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "            \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "            'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "            'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "            'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n",
    "            'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', \n",
    "            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "            'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "            'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', \n",
    "            'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', \n",
    "            'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', \n",
    "            'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', \n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', \n",
    "            'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', \n",
    "            'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", \n",
    "            'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", \n",
    "            'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "            'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n",
    "            'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "            \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "# Add extras you want to remove here\n",
    "custom_stops = ['subject', 'from', 'to', 'article', 'summary','nntp', 'posting',\n",
    "                'host' ]\n",
    "# Join two lists\n",
    "our_stopwords = stopwords + custom_stops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "# Lower case\n",
    "df_text_ng['text_lower'] = df_text_ng['text'].str.lower()\n",
    "# Remove \\n (newlines), and strings with emails or .com IP style addresses\n",
    "df_text_ng['text_noemails'] = df_text_ng['text_lower'].replace(r\"\\n\",\" \",\n",
    "                              regex=True).replace(r\"\\S*[@.]\\S*\\s?\",\"\", regex=True)\n",
    "# Remove anything except letters and spaces\n",
    "df_text_ng['text_punct'] = df_text_ng['text_noemails'].replace(r\"[^a-z ]\",\" \",\n",
    "                              regex=True)\n",
    "# Split string into lists of words on the whitespace\n",
    "df_text_ng['text_tokens'] = df_text_ng['text_punct'].str.split()\n",
    "# Remove words not in our prederived list\n",
    "df_text_ng['text_stop'] = df_text_ng['text_tokens'].apply(lambda x: [word \n",
    "                             for word in x if word not in our_stopwords])\n",
    "# Rejoin into a string \n",
    "df_text_ng['text_clean'] = df_text_ng['text_stop'].apply(lambda x: ' '.join([word for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original record 1: \n",
      "\n",
      "\n",
      "0    From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\n",
      "Name: text, dtype: object\n",
      "\n",
      "\n",
      "Cleaned record 1: \n",
      "\n",
      "\n",
      "0    thing car organization university maryland college park lines wondering anyone could enlighten car saw door sports car looked late early called doors really addition front bumper separate rest anyone tellme model name engine specs years production car made history whatever info funky looking car please thanks il brought neighborhood lerxst\n",
      "Name: text_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Original record 1: \\n\\n\")\n",
    "print(df_text_ng['text'][0:1])\n",
    "print(\"\\n\")\n",
    "print(\"Cleaned record 1: \\n\\n\")\n",
    "print(df_text_ng['text_clean'][0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Below we'll create the three possible sets that may be used (There are other <br>methods\n",
    "but these are the voting options) <br>\n",
    "<ol>\n",
    "<li> Set 1: Bag of words with Unigrams\n",
    "<li> Set 2: Bag of words with Unigrams and bigrams\n",
    "<li> Set 3: TF-IDF with unigrams\n",
    "</ol> <br>\n",
    "Uncomment the set you would like to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "First 20 features:....\n",
      "\n",
      "['ab', 'able', 'abraham', 'abs', 'absolute', 'absurd', 'abuse', 'acceleration', 'acceptance', 'access', 'accessories', 'accident', 'accidental', 'accidentally', 'accidents', 'accuracy', 'achieved', 'acquisition', 'acrv', 'active']\n"
     ]
    }
   ],
   "source": [
    "#Uncomment this cell for Set 1\n",
    "bogS1 = CountVectorizer()\n",
    "X = bogS1.fit_transform(df_text_ng['text_clean'])\n",
    "print(X.toarray())\n",
    "print(\"First 20 features:....\\n\")\n",
    "print(bogS1.get_feature_names()[0:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 features:....\n",
      "\n",
      "['ab', 'able', 'able amplify', 'able import', 'able point', 'abraham', 'abraham moses', 'abs', 'abs security', 'absolute', 'absolute best', 'absolute gets', 'absolute moral', 'absurd', 'absurd scsi', 'abuse', 'abuse tiff', 'acceleration', 'acceleration clock', 'acceleration higher']\n"
     ]
    }
   ],
   "source": [
    "#Uncomment this cell for Set 2\n",
    "bogS2 = CountVectorizer(ngram_range=(1,2))\n",
    "X = bogS2.fit_transform(df_text_ng['text_clean'])\n",
    "print(\"First 20 features:....\\n\")\n",
    "print(bogS2.get_feature_names()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 features:....\n",
      "\n",
      "['ab', 'able', 'abraham', 'abs', 'absolute', 'absurd', 'abuse', 'acceleration', 'acceptance', 'access', 'accessories', 'accident', 'accidental', 'accidentally', 'accidents', 'accuracy', 'achieved', 'acquisition', 'acrv', 'active']\n"
     ]
    }
   ],
   "source": [
    "#Uncomment this cell for Set 3\n",
    "tfidfvect = TfidfVectorizer()\n",
    "X = tfidfvect.fit_transform(df_text_ng['text_clean'])\n",
    "print(\"First 20 features:....\\n\")\n",
    "print(tfidfvect.get_feature_names()[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA...\n",
      "LDA duration: 0\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "print(\"Performing dimensionality reduction using LSA...\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components, random_state=0, evaluate_every=1)\n",
    "lda_model = lda.fit(X)\n",
    "X_DR = lda.fit_transform(X)\n",
    "\n",
    "end = time.time()\n",
    "duration = np.round(end - start)\n",
    "print(\"LDA duration: %i\" %(duration))\n",
    "# plt.plot(X_DR.max(axis=1),'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA...\n",
      "LDA duration: 0\n",
      "Explained variance of the SVD step: 100%\n"
     ]
    }
   ],
   "source": [
    "# LSA\n",
    "## Prepare data for clustering\n",
    "\n",
    "print(\"Performing dimensionality reduction using LSA...\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# # Vectorizer results are normalized, which makes KMeans behave as\n",
    "# # spherical k-means for better results. Since LSA/SVD results are\n",
    "# # not normalized, we have to redo the normalization.\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X_array = X.toarray()\n",
    "X_DR = lsa.fit_transform(X)\n",
    "\n",
    "end = time.time()\n",
    "duration = np.round(end - start)\n",
    "print(\"LDA duration: %i\" %(duration))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "     int(explained_variance * 100)))\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(svd.n_components)\n",
    "\n",
    "plt.bar(features, svd.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('LSA features')\n",
    "plt.ylabel('variance %');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using PCA...\n",
      "Explained variance of the PCA: 99%\n",
      "LDA duration: 0\n",
      "The shape of X_DR is:  (20, 20)\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "\n",
    "## Prepare data for clustering\n",
    "\n",
    "print(\"Performing dimensionality reduction using PCA...\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_DR = pca.fit_transform(X.toarray())\n",
    "principalDf = pd.DataFrame(data = X_DR)\n",
    "\n",
    "print(\"Explained variance of the PCA: {}%\".format(\n",
    "     int(pca.explained_variance_ratio_.sum() * 100)))\n",
    "\n",
    "end = time.time()\n",
    "duration = np.round(end - start)\n",
    "print(\"LDA duration: %i\" %(duration))\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %');\n",
    "print('The shape of X_DR is: ', X_DR.shape)\n",
    "#plt.xticks(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Modelling Parameters\n",
    "\n",
    "## Number of CLusters\n",
    "K = 2\n",
    "\n",
    "## Method for Initialisation\n",
    "init='k-means++' ## k-means++ selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.\n",
    "\n",
    "## The number of initializations to perform. The best results are kept.\n",
    "n_init=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create model instance\n",
    "model_instance = KMeans(n_clusters = K,\n",
    "                        init=init,\n",
    "                        n_init=n_init\n",
    "                       )\n",
    "\n",
    "## Fit data to model\n",
    "model = model_instance.fit(X_DR)\n",
    "\n",
    "## Get labels for evaluation\n",
    "labels = model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Modelling Parameters\n",
    "\n",
    "## Number of CLusters\n",
    "K = 5\n",
    "\n",
    "## String describing the type of covariance parameters to use.\n",
    "covariance_type='full' ## Full means each component has its own general covariance matrix\n",
    "\n",
    "## The number of initializations to perform. The best results are kept.\n",
    "n_init=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create model instance\n",
    "model_instance = GaussianMixture(n_components=K\n",
    "                                , n_init=n_init\n",
    "                                , covariance_type=covariance_type\n",
    "                                )\n",
    "\n",
    "## Fit data to model\n",
    "model = model_instance.fit(X_DR)\n",
    "\n",
    "## Get labels for evaluation\n",
    "labels = model.predict(X_DR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Modelling Parameters\n",
    "\n",
    "## The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "eps = 1.3\n",
    "\n",
    "## The number of samples in a neighborhood for a point to be considered as a core point. This includes the point itself.\n",
    "min_samples = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create model instance\n",
    "model_instance = DBSCAN(eps=eps\n",
    "                        , min_samples=min_samples\n",
    "                        )\n",
    "\n",
    "## Fit data to model\n",
    "model = model_instance.fit(X_DR)\n",
    "\n",
    "## Get labels for evaluation\n",
    "labels = model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004528616739895276\n",
      "2.069880711696186\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "## Silhouette Score\n",
    "silhouette_score = metrics.silhouette_score(X_DR\n",
    "                                            , labels\n",
    "                                           )\n",
    "print(silhouette_score)\n",
    "\n",
    "## Davies-Bouldin Score\n",
    "davies_bouldin_score = metrics.davies_bouldin_score(X_DR\n",
    "                                                    , labels\n",
    "                                                   )\n",
    "\n",
    "print(davies_bouldin_score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7dd336b956e436c3e4239a1607ccc59572089974d552a2947c38e44483f0f2ff"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
